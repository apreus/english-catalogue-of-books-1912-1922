{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013561a1-d896-485a-848d-30b0b9433b6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import regex as re\n",
    "from pathlib import Path\n",
    "from Levenshtein import distance\n",
    "\n",
    "current_dir = Path.cwd()\n",
    "parsed_dataframe_directory = current_dir.parent / 'dataframes' / 'parsed_dataframes'\n",
    "file_location = parsed_dataframe_directory / 'all_entries.csv'  # combined output from llm_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e91238dc-3e5c-4c79-9364-19a850272419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# quick sanity check: entry counts per year\n",
    "parsed_dataframe_directory = current_dir.parent / 'dataframes' / 'parsed_dataframes'\n",
    "\n",
    "for item in sorted(parsed_dataframe_directory.iterdir()):\n",
    "\n",
    "    if item.name.startswith('.') or item.name.startswith('all') or item.is_dir():\n",
    "        continue\n",
    "\n",
    "    file_name_regex = r\"entries_[0-9]{4}\"\n",
    "    file_name_match = re.findall(file_name_regex, str(item.name))\n",
    "    if not file_name_match:\n",
    "        print(f\"⚠️ Skipping file with unexpected name format: {item.name}\")\n",
    "        continue\n",
    "\n",
    "    file_name = file_name_match[0] + \".csv\"\n",
    "    df = pd.read_csv(parsed_dataframe_directory / file_name)\n",
    "    print(f\"{file_name_match[0]}: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49afb8a4-46fc-4fa9-a885-a84eed96bee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def entry_clean(entry):\n",
    "    \"\"\"Stitch parsed fields back into a single string for comparison.\"\"\"\n",
    "    fields = ['opening_bits', 'author(s)', 'title', 'format', 'little_bits', 'publisher', 'date']\n",
    "    parts = [str(entry[field]) if pd.notna(entry[field]) else ' ' for field in fields]\n",
    "    stitched = ' '.join(parts).strip()    \n",
    "    return stitched\n",
    "\n",
    "def strip_string_levenshtein(entry):\n",
    "    \"\"\"Remove all non-alphanumeric chars for levenshtein comparison.\"\"\"\n",
    "    if not isinstance(entry, str) or pd.isna(entry):\n",
    "        return ''\n",
    "    return re.sub(\"[\\W]\",\"\",entry).lower()\n",
    "\n",
    "def strip_string(entry):\n",
    "    \"\"\"Normalize whitespace/punctuation for jaccard comparison.\"\"\"\n",
    "    return \" \".join(re.sub(r\"\\W\", \" \", entry).split())\n",
    "\n",
    "def jaccard_similarity(str1, str2):\n",
    "    \"\"\"Token-level jaccard similarity between two strings.\"\"\"\n",
    "    set1 = set(str1.split())\n",
    "    set2 = set(str2.split())\n",
    "    intersection = len(set1.intersection(set2))\n",
    "    union = len(set1.union(set2))\n",
    "    return intersection / union if union != 0 else 0\n",
    "\n",
    "def compute_row_scores(row):\n",
    "    \"\"\"Compare original entry vs stitched parsed fields.\n",
    "    Uses levenshtein for char-level diff, jaccard for token-level.\n",
    "    Flags entries where parsing lost or added content.\"\"\"\n",
    "    stiched_entry = entry_clean(row)\n",
    "    \n",
    "    original_str_lev = strip_string_levenshtein(str(row['original_entry']))\n",
    "    stiched_entry_lev = strip_string_levenshtein(stiched_entry)\n",
    "    lev_score = distance(original_str_lev, stiched_entry_lev)\n",
    "    \n",
    "    jaccard_score = None\n",
    "    if lev_score > 1:\n",
    "        original_str_jac = strip_string(str(row['original_entry'])).lower()\n",
    "        stiched_entry_jac = strip_string(stiched_entry).lower()\n",
    "        jaccard_score = jaccard_similarity(original_str_jac, stiched_entry_jac)\n",
    "    else:\n",
    "        jaccard_score = 1.0\n",
    "    \n",
    "    # flag if levenshtein >= 1 AND jaccard <= 0.99\n",
    "    flagged = (lev_score >= 1) and (jaccard_score <= 0.99)\n",
    "    \n",
    "    diff = token_diff(str(row['original_entry']), stiched_entry)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'levenshtein': lev_score,\n",
    "        'jaccard': jaccard_score,\n",
    "        'flagged_for_correction': flagged,\n",
    "        'diff': diff\n",
    "    })\n",
    "\n",
    "def token_diff(a, b):\n",
    "    \"\"\"Show which tokens are shared vs only in original/parsed.\"\"\"\n",
    "    a_clean = strip_string(a)\n",
    "    b_clean = strip_string(b)\n",
    "    \n",
    "    a_tokens = set(a_clean.split())\n",
    "    b_tokens = set(b_clean.split())\n",
    "    \n",
    "    shared = a_tokens & b_tokens\n",
    "    only_in_a = a_tokens - b_tokens\n",
    "    only_in_b = b_tokens - a_tokens\n",
    "\n",
    "    return {\n",
    "        'shared': sorted(shared),\n",
    "        'only_in_a': sorted(only_in_a),\n",
    "        'only_in_b': sorted(only_in_b),\n",
    "        'jaccard': len(shared) / len(a_tokens | b_tokens) if a_tokens | b_tokens else 1.0\n",
    "    }\n",
    "\n",
    "def print_entry(entry):\n",
    "    \"\"\"Pretty-print a single entry for inspection.\"\"\"\n",
    "    print(f\"{entry['original_entry']}\")\n",
    "    print(\"- - - - -\")\n",
    "    print(f\"opening_bits: {entry['opening_bits']}\")\n",
    "    print(f\"author(s): {entry['author(s)']}\")\n",
    "    print(f\"title: {entry['title']}\")\n",
    "    print(f\"format: {entry['format']}\")\n",
    "    print(f\"little_bits: {entry['little_bits']}\")\n",
    "    print(f\"publisher: {entry['publisher']}\")\n",
    "    print(f\"date: {entry['date']}\")\n",
    "    print(f\"levenshtein: {entry['levenshtein']}\")\n",
    "    print(f\"jaccard: {entry['jaccard']}\")\n",
    "    print(f\"diff: {entry['diff']['shared']}\")\n",
    "    print(f\"only in original entry: {entry['diff']['only_in_a']}\")\n",
    "    print(f\"only in parsed entry: {entry['diff']['only_in_b']}\")\n",
    "    print(\"–––––––\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ebb9c8-4683-45a8-9341-eabbde3c0fdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66445951-d2f3-420b-a494-dc35ad848bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# score every row: levenshtein, jaccard, flagged, token diff\n",
    "df[['levenshtein', 'jaccard', 'flagged', 'diff']] = df.apply(compute_row_scores, axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395bea5-7592-4d5f-b8bc-3d210eb889dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# also flag entries missing both author+title, or missing publisher\n",
    "no_author_or_title = (df['author(s)'].isna()) & (df['title'].isna())\n",
    "df.loc[no_author_or_title, 'flagged'] = True\n",
    "\n",
    "no_publisher = df['publisher'].isna()\n",
    "df.loc[no_publisher, 'flagged'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1jevnt5eq0s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary stats\n",
    "print(f\"Total entries: {len(df)}\")\n",
    "print(f\"Flagged: {df['flagged'].sum()} ({df['flagged'].mean()*100:.1f}%)\")\n",
    "print(f\"\\nLevenshtein > 0: {(df['levenshtein'] > 0).sum()}\")\n",
    "print(f\"Jaccard < 1.0: {(df['jaccard'] < 1.0).sum()}\")\n",
    "print(f\"Jaccard < 0.9: {(df['jaccard'] < 0.9).sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zmf0wn4op2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect low-accuracy entries (sorted worst first)\n",
    "low_accuracy = df[df['flagged'] == True].sort_values('jaccard')\n",
    "print(f\"{len(low_accuracy)} flagged entries\\n\")\n",
    "\n",
    "for _, entry in low_accuracy.head(20).iterrows():\n",
    "    print_entry(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94py9zzyb1l",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export flagged entries for manual review\n",
    "low_accuracy.to_csv('flagged_entries.csv', index=True)\n",
    "print(f\"Exported {len(low_accuracy)} flagged entries to flagged_entries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "438fc70f-aec7-4fc9-95bb-8a34cdd3fe05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# keep only the core columns, drop score columns\n",
    "# new_column_order = ['original_entry', 'opening_bits', 'author(s)', 'title', 'format', 'little_bits', 'publisher', 'date', 'catalogue_year', 'page_num', 'doc_page_num']\n",
    "# df = df.reindex(columns=new_column_order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c24a4199-afb9-473c-b015-9166b5a42ca9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save final mega list\n",
    "# df.to_csv('parsed_entries_mega_list.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
